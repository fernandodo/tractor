**Tumaati Rameshtrh<sup>1</sup>[\*](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#cor1), Anusha Sanampudi<sup>1</sup>, S. Srijayanthis<sup>1</sup>, S. Vijayakumarsvk<sup>1</sup>, Vijayabhaskar<sup>1</sup> and S. Gomathigomathi<sup>2</sup>**

_<sup>2</sup>Sairam Engineering College, Tamil Nadu, India_

### _Abstract_

The goal of this research is to catalog the myriad of emotions that are experienced by people and to determine a person’s state of mind by observing his or her behavior and drawing conclusions from those observations. Anger, sadness, fear, pleasure, disgust, surprise, and neutral are some of the facial emotion classes that may be recognized by utilizing facial expression recognition, often known as FER. Finding the database in the emotion by applying the Fluorescence-activated cell sorting (FACS) action unit (AU), such as impression investigation, diagnosing depression and behavioral disorders, lying detection, and (hidden) emotion identification, among other things. A deep convolutional neural network (CNN) approach was used in order to realize the goal of recognizing different facial expressions. The technique that was recommended puts a significant focus on achieving a high degree of accuracy when determining the feelings that are communicated in live video footage.

**_Keywords_:** Image processing, facial expression, emotion recognition, deep learning, autonomous vehicle

## 8.1 Introduction

With the use of FACS, we are able to determine a person’s mental condition and provide appropriate recommendations. This analysis of different facial expressions provides a number of methods for evaluating current emotions in the here and now (facial electromyography \[FEMG\] is another option). Research that was carried out by Paul Ekman in the early 1970s led to the discovery of six different facial expressions that are common to all human cultures. Disgust, wrath, happiness, grief, surprise, and fear are some of the feelings \[[1](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref1)\] that fall under this category. These behaviors could be discovered after doing a thorough investigation of the profile indicator. For example, bringing the inner corners of one’s eyes together while simultaneously elevating the corners of one’s lips might be seen as a sign of pleasure. Facial expression recognition (FER), which is one of the most practical methods to integrate nonverbal information with various types of analytics due to the fact that it may provide some insight into a person’s emotional state as well as his or her objectives, is one of the methods to integrate nonverbal information. The only assignment that can be finished using our suggested technique, which is based on convolutional neural network (CNN)-based attribute styles \[[2](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref2)\], is finding out the emotions that each person in the video clip is experiencing throughout the different frames. This is the only task that can be done using our suggested strategy. We are currently gathering the data from the video clip in order to figure out the feelings experienced by each participant. The number of heads that are necessary to have specific knowledge is dropping and so is the number of variables that need to be skillfully worked out. In addition, the number of heads that are required to have certain information is falling. It is possible that the algorithm may have been enhanced if it had been carried out in a different manner. This article presents a method for identifying the FER that is centered on CNN \[[3](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref3)\], and it is presented by us to the reader. As the input, the video or movie clip is utilized, and then CNN is used to observe human activities and their behavior. The Virtual Focal Reality (VFR) technology is a kind of computer vision technology that collects considerable data about the video sequences that are included inside a single frame. This information may then be analyzed. When the VFR system is operational, a variety of businesses, including RFID \[[4](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref4)\], will be interested in the field of system recognition and computer vision due to the widespread application of RFID technology in the fields of (radio frequency identification) cards, smart cards, surveillance systems, pay systems, and access control. When the VFR system \[[5](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref5)\] is operational, RFID will also be interested in the field of system recognition and computer vision.

The development of autonomous vehicles (AVs) \[[6](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref6)\] has taken place in phases over the course of the last couple of decades using a wide range of technology. These may include proximity warning systems, monitoring of the vehicle’s operation, autonomous cruise control, detection and avoidance of impediments on the road, management of the brakes, aid with driving and parking, monitoring of the vehicle’s operation, and more. In this chapter, the concept of “automated vehicle parking” is dissected into its component elements in order to better understand it. The ability of a vehicle to park itself without any assistance from a human from its starting location all the way to its destination position while maintaining the correct orientation is the definition of autonomous vehicle parking. This capability must be maintained while the vehicle is traveling from its starting location to its destination position. The many ways that automobiles may be parked are outlined in the following list: • Garage Parking • Parallel Parking • Diagonal Parking. When a car is parked parallel to the road or the curb, or when it is situated in between two other parked cars, we say that the automobile is parked parallel. One of the several kinds of parking is called parallel parking \[[7](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref7)\]. In the majority of situations, you will be able to locate this kind of parking on streets, highways, crowded areas, and other comparable sites. Other such places include the following: When it comes to parking, parallel parking is often considered to be one of the most difficult kinds of parking, even for experienced human drivers. Parking in forward parallel or parking in reverse parallel are the two approaches that may be used to complete this assignment. From [Figure 8.1](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#fig8-1) researchers are having a difficult time developing an automatic parallel parking system since the available parking space for this kind of parking is either very limited or entirely unknown. When the area that the vehicle is working in is very dynamic \[[8](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref8)\], it makes it a lot more challenging; as a consequence, the vehicle has to be furnished with an adequate feedback system. According to the most current trends, the automobile industry seems to be heading toward the use of much more automated procedures. Currently, a number of different automakers are actively toiling away at the creation of an extensive range of various types of driverless cars. It is hoped that an autonomous vehicle would behave in a responsible manner and be able to negotiate difficulties with just the slightest amount of help from a human driver. It is reasonable to anticipate that an automobile with autonomous capabilities will be able to park itself. Both the infrastructure for such a system and the building of a prototype \[[9](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref9)\] for an autonomous parking system are now the subject of a substantial amount of research that is currently being carried out. It will be very challenging for a self-driving vehicle to achieve a level of expertise and capability that is equivalent to that of a human driver who has years of experience. Every algorithm for machine learning works toward the ultimate aim of reaching a point where human intelligence and machine intelligence are on an equal footing. In order to properly fulfill the task of parking an autonomous vehicle, an intelligent onboard technology is also required. In addition to this, learning how to park the automobile may be difficult for novice drivers, and human error can be the root of a range of driving-related problems. The convenience and peace of mind that autonomous parking technology provides to human drivers are well worth the investment. In addition to this, it cuts down on the amount of time that is wasted sitting in traffic, which in turn decreases the quantity of fuel that is used. However, the system must be designed with a sufficient amount of care; otherwise, it runs the danger of failing, putting the lives of the passengers at hazard. When it comes to parking, the majority of the solutions that are now available exclusively concentrate on the static environment, and they approach the problem of automobile parking as a distinct obstacle. The work that is being recommended extends upon car parking challenges by adding the navigation module in order to produce solutions that are robust and more acceptable in settings of dynamic complexity. This is done in order to generate more workable alternatives. • Our algorithms, which are based on fuzzy logic \[[10](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref10)\] and neural networks, take into consideration the dynamic character of the environment, which is caused by the existence or arrival of any live or nonliving object or hindrance. This may be an item or an impediment of any kind. • The study that we have recommended seeks to further integrate the dimensionality of the vehicle, or the capacity to park automobiles of varied sizes, and it does so by using flexible and behavior-based reasoning \[[11](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref11)\]. • A fuzzy-based parallel parking controller was constructed initially and was based on the fifth-order polynomial. This controller was designed to allow an automobile to be parked in a parallel position in a predetermined parking slot in either the forward or the reverse direction, depending on the starting and ending states of the vehicle. • After successfully implementing the parking algorithm based on traditional path planning, we extend our work to incorporate dynamicity in the environment in order to avoid collisions while still keeping track of the destination. This was accomplished by incorporating dynamicity in the environment in order to achieve this goal. The incorporation of dynamic elements \[[12](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref12)\] into the environment helped achieve this goal by reducing the likelihood of collisions occurring. 1. Having taken into consideration the following:

![[attachments/fig8-1.jpg]]

[**Figure 8.1**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rfig8-1) Flowchart for the testing phase.

We developed a navigation system that is based on fuzzy logic and may redirect the vehicle based on the information that is received from the various sensors. This is useful in situations in which an item restricts the parking path.

•A one-of-a-kind method for calculating ultrasonic range is developed, and it is put to use in order to provide the navigation controller with information (sensing) on the surroundings all around it. This is done in order for the navigation controller to understand where it is in relation to its surroundings. The approach that is now under discussion is capable of simulating actual environmental sensing and calculates the distance between sensors depending on the form of the vehicle. • Unlike previous algorithms, we addressed the problem of navigation as an extension of the parallel parking task.

This allowed us to solve both problems simultaneously. It has been made possible for the driver to steer the vehicle to the parking area from any starting position—thanks to a system that has been devised. After the vehicle has completed its navigation and has arrived at its parking spot, a parallel parking controller will assume control of the vehicle. After that, it parks the car inside the predetermined space while ensuring that it is facing the right direction. In addition to a controller designed for parallel parking, this system made use of a controller designed specifically for navigation. • As a result of this, a hybrid model that is composed of a navigation controller and a controller for parallel parking is provided as a solution for handling the scenarios that include a large number of parking places \[[13](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref13)\]. When it is believed that the vehicle has arrived at the first available parking place and occupied that position, the process is considered to have begun. Initially, it has no awareness of the available parking places and is completely clueless about them. The process starts with parallel parking, and ultrasonic sensors are utilized to assess whether or not a parking place is occupied at that particular location. If the automobile determines that there is already another vehicle using the parking spot, the parking procedure is canceled, and the navigation controller \[[14](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref14)\] is used in its stead. The navigation controller will drive the vehicle to the next parking location that is available, and this process will continue until there is a parking spot that can be used. When the vehicle finds a location in which it may park without being blocked by another vehicle, parking is regarded to be complete. • As a solution to our first problem, which was the ability to park the vehicle in a dynamic environment, we proposed a novel parallel parking architecture in a dynamic environment with the integration of an obstacle avoidance fuzzy controller. This was done in order to meet the requirements of our first challenge. Both the forward and the reverse parking strategies are effective when used in this design, which works well for N number of parking places. The obstacle avoidance controller and the fifth-order polynomial-based parking controller are brought together and combined as part of this novel solution. We made use of a third controller that was known as the “decision” controller. The purpose of this controller was to direct the workflow of the other two controllers.

During the course of the parking process, establishing this system with the intention of identifying any automobile or obstacle that was not anticipated was the primary goal. The path that the car takes is intelligently changed when the sensors of the vehicle identify an obstruction, but the destination of the vehicle is not forgotten in the process. The vehicle makes its route selection throughout this process of rerouting by taking into consideration the amount of space that is available between the congested areas. In order to determine whether or not an intelligent option from a system may effectively avert a collision, this architecture is put through its paces with both fixed and moving impediments in a range of different circumstances. In addition, we investigated multistage procedures, and as a consequence of our research, we provided a neuro-fuzzy architecture for parking in dynamic situations. This was accomplished as a result of the findings of our study. It has been found that making improvements to the fuzzy system \[[15](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref15)\] by adding neural networks as a prefix can result in enhanced performance while parking autos.

The findings of the simulation indicate that it is effective in overcoming the limitation of ambiguity that is inherent in a fuzzy-based system with a single stage due to the fact that it avoids collisions. This is shown by the fact that it was able to do so. As a response to our second obstacle, which was the capacity of parking cars of varied sizes, we came up with a revolutionary fuzzy-based adaptive dimension parking method. This technique was suggested by us as a solution. This algorithm is able to adjust its threshold for the behavior controller and carry out its ideal route while taking into consideration the dimensions of the vehicle. Scaling factors are determined and calculated depending on the dimensions of the vehicle that are supplied in the description. After that, the fuzzy \[[16](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref16)\] variable thresholds that are utilized in the obstacle avoidance module are adjusted based on these scaling factors that are employed. In addition, the application of the artificial potential field theory is used during the course of the runtime in order to make these thresholds malleable and adjustable. This algorithm was developed because a parking controller designed for one set of fixed-sized automobiles, such as hatchbacks, sedans, or SUVs, was unable to work efficiently with another set of fixed-sized vehicles, such as convertibles. This prompted the development of this algorithm. This approach has been examined using the measurements of a Range Rover and a Hyundai i20.

## 8.2 Related Work

### 8.2.1 Face Detection

CNN used a range of drop sizes in its investigation of the effectiveness of models for facial recognition, which may be found in this article. The subsequent system, which was utilized in our case studies, carried out an analysis of the structure by using the following criteria: The M convolutional layers, also known as the spatial volume normalization (SBN) layers, are what are meant to be referred to when using the phrase “beginning area” in the early stages of the phase. In addition to dropout and maximum pooling, these surfaces are guaranteed to be there at all times. The system always results in combined layers, which are always found after the M convolution layers that it has previously analyzed \[[17](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref17)\]. In their communications, they encounter nonlinearity and offline functioning, as well as volume normalization (BN) and dropout. The offline layer, which comes in at number 3 on our list, is the one that is in charge of generating the scores and the smooth-maximum loss function after the network. The advanced model is concerned with the volume normalization and the number of complete and connected layers per user, and it also gives the capability to assess whether or not there is a drop and how many layers may pool to their maximum. All of these factors contribute to the normalization of the volume. In addition to methods for dropout and volume normalization, our process included L2 modulation as one of its steps. In addition, the user has the possibility to specify the number of filters, upgrades, and zero-fill \[[18](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref18)\]; however, in the case that these values are not specified, default values will be taken into consideration as shown in [Figure 8.2](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#fig8-2).

![[attachments/fig8-2.jpg]]

[**Figure 8.2**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rfig8-2) Different types of facial emotions.

### 8.2.2 Facial Emotion Recognition

The three basic processes that make up the process of facial recognition are known as face detection, CNN feature extraction, and facial modeling \[[19](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref19)\] ([Figure 8.3](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#fig8-3)). In order to clarify, the functioning of the system may be broken down as follows: It has the ability to obtain input n clips from the databases that correspond to those clips.

It is possible for it to park itself automatically in a parallel manner with a given initial and final position; however, if there is any uncertainty or unanticipated event, this control design will cause the vehicle to collide if it is used on its own and if it is used as a stand-alone. This is provided if all environmental conditions are clear enough.

In order for a modern parking system to be considerate of its environment, it is necessary for it to have a sensing mechanism that is incorporated into the parking controller.

When it comes to parking, a human driver with a lot of experience can perform the necessary steering maneuvers to park a car by quickly evaluating the vehicle’s \[[20](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref20)\] end position in addition to its orientation in the ideal parking condition. This is possible because the human driver is able to visualize both the ideal parking condition and the end position of the vehicle. On the other hand, in order for autonomous robots to recreate the same piece of artwork, a sensory system that is adequate is necessary. It is possible that a vision system is an excellent duplicate of the human eye. Despite this, it is a fact that such sensing provides only a vast field of view, and it is possible for them to completely overlook a very limited range of obstacles such as corners, the face of other cars, or the walls themselves. One option to lessen the impact of this problem is to make use of active sensors, such as ultrasonic sensors, infrared sensors, laser sensors, and so on, among other types of active sensors.

In this section, we are going to talk about a one-of-a-kind geometric method that will allow us to get a better sense of the distance information from the boundary of the Contrastive Learning of Musical Representations (CLMR). This information that has been perceived is really simulated data, but they may be contrasted with real-time sensor data that were acquired by any active sensors \[[21](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref21)\].

The devices known as ultrasonic sensors are advantageous in that they are not only affordable but also provide a direct range of data regardless of the environment in which they are located. Furthermore, in contrast to infrared sensors, the accuracy of the detection performed by ultrasonic sensors is not affected by the color of the object \[[22](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref22)\] that is being detected. This is an advantage that ultrasonic sensors have over infrared sensors. Because of this, the sensing technique will only be explained in relation to ultrasonic sensors throughout this whole article. The work that we have done involves combining the data obtained from ultrasonic sensors with those of multilevel fuzzy controllers in order to complete the multifunctional task.

The next step is to find the face in the photograph and then delete it from the file. After that, the facial images are sent as a capture to a neural network \[[23](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#ref23)\] that may be updated in order to extract the compatibility qualities of the faces in the photographs. The real work will eventually be between modeling that is carried out by an algorithm for machine learning at some point in the future.

## 8.3 Proposed Method

In order to assess human emotions, this research used the following features: SVM, Gabor Feature, Histogram of Oriented Gradients, Mask-CNN, and Local Binary Pattern. For the purpose of putting the theory to the test, it is necessary to isolate the human body and mind from the movie. In the process of assessing human mood, well-known machine learning algorithms such as SVM, VFR, RF, and K-NN \[24–27\] have all been used at various points in the investigation. Four different CNN models, namely, Alex Net, VGG-19, ResNet-50, and Inception-ResNet-v2, are selected to be used in the process of determining the temperature of the inside of the body. These CNN models have been trained on ImageNet2 to differentiate between pictures of people and pictures of nature with an exceptional level of accuracy, and these models are the ones that are utilized. LBP and SVM have provided the most accurate spectrum of emotions that may be used in real time. Both of these methods have been successful. The use of neural networks was necessary in order to attain this goal.

The picture from the input video was extracted and preprocessed at the beginning of the procedure. After that, the CNN approach was used to extract the features. After removing all of the characteristics, the data were categorized using the Softmax classifier model.

### 8.3.1 Dataset

The Kaggle website, which provides more than 37,000 well-trained grayscale snapshots of profile images at a resolution of 48 **×** 48 pixels, was scoured for datasets to compile this article. The images have been meticulously curated and nearly give off the impression of being positioned in the same spot. From [Figures 8.3](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#fig8-3) to [8.5](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#fig8-5), each one of the distinct profiles has provided the exact same amount of scope in every clip. Every single photograph in the training programmed has to be assigned to one of seven different states of mind, each of which is denoted by a different look on the subject’s face. These facial expressions of emotion may be categorized according to a number of different states, including anger, hatred, fear, pleasure, grief, surprise, and neutrality. More than 29,000 images have already been produced, 4,000 photographs are to be used as evidence, and 4,000 snapshots are used for instructional reasons. Following further analysis of the raw pixel data, the exact and tested images were chosen from each and every trained image by first excluding the training photographs and then selecting the images from each and every trained image. During the process of developing the data augmentation, similar images were patched together and used for the purpose of putting attention on the horizontal train set. Characteristics make use of the fundamental raw pixel data that were used to update the features that were generated by layers. These features have been put to use in the process of forming expressions. Learning models were built in order to make it easier to conduct more research. These models were constructed out of HOG features, which were then changed to connect the layers together before being combined into fully integrated input feature (FC) layers.

![[attachments/fig8-3.jpg]]

[**Figure 8.3**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rfig8-3) Segmentation of the Facial Expression Recognizer (FER) method. Respective sections are sketched in detail in its comparable section.

![[attachments/fig8-4.jpg]]

**Figure 8.4** Preprocessing and extraction/classifier of frame using CNN.

![[attachments/fig8-5.jpg]]

[**Figure 8.5**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rfig8-5) System architecture for real emotion recognition using movie clips.

### 8.3.2 Preprocessing

Because the size of the picture has already been decided, the original photograph has to be reduced in size before it can be used as an input. This is because the size of the image has already been determined. It was point (x, y) in the very first snapshot that was given to us, and it is this point that has to be expressed and plotted to become point (x’, y’). Where SX indicates the computing ratio of the picture when viewed in the direction of the x-axis, and see represents the scaling ratio of the image when seen in the direction of the y-axis. A, B, C, and D are the letters that stand in for the picture’s individual pixels (x, y). As it moves through the state, the provided clip’s dimensions are gradually shrunk until they equal 128 **×** 128.

### 8.3.3 Grayscale Equalization

When snapped together, they expose a seemingly haphazard distribution of light and shade, making the information difficult to decipher. It is not at all difficult to locate the brilliant spots, trails, and other elements that were produced by the activity in the actual image that was recorded. It is essential to normalize the gray scale of the image in order to get the maximum amount of contrast in the picture that is physically achievable. In order to proceed with the snapping, the Histogram Equalization (HE) model was used in the data in this circumstance. The primary objective here is to convert the map from the original map into a distribution that is uniform all the way through. If the gray level of the film is L, then its magnitude will be M > N, and the number of bytes that make up its gray level will be E. If the gray level of the film is L, then these values will all be true.

The decay of the snap has the highest magnitude, and the contrast of the snap has the largest magnitude, when the comic histogram of the image is precisely the same. Therefore, gray-level uniformity acknowledges that the even dispersion of the snap histogram is good for obtaining the facial features. This is because it enhances the contrast of the snap and offers clear clarity. In order to get the particulars of the snap edge, the Kirsch edge speculation method is used.

## 8.4 Results and Analysis

The development of a model is going to be the key focus of the system that has been suggested to be used. The system had three distinct surfaces: two of which were curved and one that was flat and round. The transition layer is located on the initial surface, and it is constructed from the components listed below in the order in which they are listed: 35 4 × 4 puzzles with a stem size of 1, including both state and foreigner groups; nonetheless, the maximum was only obtained by not using any pooling tactics at any point in the process. From [Figure 8.6](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#fig8-6) grayscale images transition layer is a jigsaw that has a dimension of 68 square inches and has a stem size of 1, drop groups, normalization, and normalization groups in addition to normalization groups. For the purpose of making the biggest pool possible, a filter with the dimensions of 2 × 2 is utilized.

In the context of this investigation, the modified linear unit will serve the purpose of representing a functional unit. Before beginning the process of training our model, we carried out a series of balancing tests in order to verify that the functions of the network are operating as intended. This was completed before we began teaching our model new skills. During the preliminary investigation of the system’s dependability, the first loss is found and documented. We have devised a system of categorization that is composed of a total of seven distinct classes, and we anticipate that the value range will be somewhere in the 1.96 region. To begin, we focused on the second balance and used just a little portion of the practice set in an attempt to track down the fit. After that, we will go on to the next step of the process, which is to train our model using the findings that we have accumulated up to this point. In order to make the process of training the model move more quickly, we made advantage of GPU-accelerated in-depth learning capabilities. Using these capabilities allowed us to complete the treatment in a timelier manner shown in [Figure 8.7](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#fig8-7).

![[attachments/fig8-6.jpg]]

[**Figure 8.6**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rfig8-6) Collections of grayscale images.

![[attachments/fig8-7.jpg]]

[**Figure 8.7**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rfig8-7) CNN architecture.

All of the pictures that make up the sample, in addition to the hyper-parameters that control the learning rate of the sample and the number of neurons that are hidden, were put through a series of cross-checks that used a wide range of different values. This was done in order to ensure that the results were accurate. A series of tests was performed on the package in order to ascertain the level of craftsmanship that went into the creation of the copy. These examinations included validating the samples at each consecutive phase of manufacturing as well as evaluating the samples at each level.

Additionally, the samples were tested at each stage. The leading superficial sample, which was assessed as part of the verification procedure and found to have an accuracy of 65%, provided 64% of the information for the package. The hyperparameters that have been tested several times for the shallow model are summarized in [Table 8.1](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#tab8-1), which offers an overview of the subject matter. This was done to guarantee that the information is correct. A deep convolutional neural network that was developed with four conversion layers and two FC layers was utilized for the training process in order to analyze the impact that transformer layers attached to the network have and to investigate the function that FC layers play. There are a total of 512 3 × 3 filters across the primary, secondary, tertiary, and final replacement layers. The primary replacement layer has 64 3 × 3 filters, the secondary replacement layer has 128 filters, the tertiary replacement layer has 512 3 × 3 filters, and the final replacement layer also has 512 filters. The size 1 volume normalization will be used in each transition layer as an activation function to initiate the construction of the layer.

[**Table 8.1**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rtab8-1) Shallow version of emotion accuracy.

| **Guidelines** | **Avails** |
| --- | --- |
| Acquirements Rate | 0.001 |
| Regulation | 1e<sup>-6</sup> |
| Invisible Neurons | 256,512,1024 |

This will take place at the beginning of the creation process. The dropout system, the max-pooling system, and the ReLu system have all seen some degree of development as a result of these developments. There are 256 neurons in the layer that cannot be seen in the main FC layers, and there were 512 neurons in the secondary FC layers as well. The layer that cannot be seen in the primary FC levels is located below the primary FC layers. We would reel in either the FC layers or the transition layers, which is where the volume normalization was situated, when we were in a rush. In case you were curious about it, the kind of loss function that we used was a Softmax. Prior to instructing students on how to use the system, initial loss checks, smaller groups of the training package, and investigations into the capabilities of better fitting the network were carried out in a manner that was comparable to that of the basic model. This was done before students were instructed on how to use the system. The results of these balance checks, which were employed in the process of activating the network, were evaluated, and it was found that they were accurate. Following that, we went on to train the network using each and every one of the images that were shown in the tutorial, making use of a total of 35 epochs and 128 volume scales throughout the process. In addition, we performed a second check on the superfluous variable in order to determine which sample had the maximum degree of accuracy. This was done in order to choose the best one.

At this juncture, 65% of the verification package has been finished, and during testing on the package, an accuracy of 64% has been achieved. The representation of this model that is contained in [Table 8.2](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#tab8-2), which has a table of the values that may be entered for each hyperparameter, is the most exact one that can be found anywhere else. Even though this did not result in an increase in the accuracy of classification, networks were trained with five and six interchangeable layers in order to explore more intricate CNNs. This was done despite the fact that this did not result in an improvement. This was carried out so that more complicated CNNs might be investigated. As a consequence of this, it was concluded that this network, which consists of two FC layers and four database levels, is the most effective choice for our database. The source pixel data are the most important components of our classification work, and this is true regardless of the complexity of the model that we use to analyze the data. This is the case regardless of whether or not we use a model with a greater level of detail. The only criteria that were taken into consideration were those that had been generated by the layers themselves. As a result of the fact that HOG properties are sensitive to boundary conditions, it is common practice to rely on them while trying to recognize facial expressions of emotion.

[**Table 8.2**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rtab8-2) Deep version of emotion accuracy.

| **Expression** | **Shallow model** | **Deep model** |
| --- | --- | --- |
| Anger | 45% | 58% |
| Disgust | 35% | 80% |
| Fear | 64% | 56% |
| Happy | 80% | 86% |
| Sad | 38% | 69% |
| Surprise | 67.5% | 62.5% |
| Natural | 40% | 52.6% |

To employ HOG features with source pixels in our network to investigate if there was any manner and to observe the performance of the model when it has a mixture of two distinct features, a new learning model was constructed using two neural networks: one for feature extraction and the other for feature combination. In the first one, some of the layers were interchangeable with others, but in the second one, all of the levels were entirely linked to one another. In order to accomplish this goal, a cutting-edge learning model was developed by combining the capabilities of two neural networks. The first neural network featured layers that could be switched out for others, but the second neural network only comprised levels that were fully connected to one another.

The characteristics that are established by the first network are combined with the HOG characteristics, and the hybrid characteristics that are formed as a consequence are made available across the second network. A superficial network and a deep network were both taught to have the same features as part of the preliminary investigation into the superficial and deep networks. This was done in order to evaluate how well the system worked when it was put to use with characteristics that were analogous to those being contrasted. At this point in the process, the correctness of the shallow model will only be evaluated with reference to the source pixels. This estimate was pretty near the accuracy that had been acquired from the shallow sample, which indicated that it was a reliable one. It should be brought to everyone’s attention that the precision of the model was the same as that of the raw pixels.

The model has been trained and validated using data obtained from the Kaggle repository. This dataset contains approximately 3,500 grayscale front-view photos of 300 human faces ranging in age from 18 to 65 years. In this particular piece of research, the model was trained using close to 480 different human faces. While the initial video clip in each and every frame sequence depicts a face that is expressionless, each subsequent clip in the series captures a different feeling and places it at the conclusion of the expression seen before. The variation in the conversation distance between all of the frames in the database is no more than 30%, which means that the minimum distance was less than 80 pixels and the maximum distance was less than 100. Therefore, as shown in [Figure 8.8](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#fig8-8), the interocular distance that was determined using the input picture is significantly less than 80 pixels or significantly more than 120 pixels.

![[attachments/fig8-8.jpg]]

[**Figure 8.8**](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rfig8-8) Matrix waveform for the sets of six global expressions.

## 8.5 Conclusions

The face expressions of the characters in this piece of literature serve to depict the emotional state of each individual character. A deep learning convolutional neural network that uses GPU technology was constructed in the system that is suggested for the purpose of collecting facial characteristics from individuals and measuring their core body temperature. It is suggested that the GPU system be used as the method to evaluate and deliver the precise range of human emotions and to extract the facial expression from the video clip. In addition, the body temperature of the individual may also be obtained. The proposed system should be able to collect data from a novel facial expression recognition dataset, which has two requirements: it must be able to recognize static video sequences and it must be able to extract face features. The proposed system should also be able to collect data from an existing facial expression recognition dataset. In addition, the core focus of this study report was on the temperature that is maintained inside an individual’s body.

## References

1.  1\. Mehmood, R.M., Du, R., Lee, H.J., Optimal feature selection and deep learning ensembles method for emotion recognition from human brain EEG sensors. _IEEE Access_, 5, 14797–14806, 2017.
2.  2\. Song, T., Zheng, W., Lu, C., Zong, Y., Zhang, X., Cui, Z., MPED: A multimodal physiological emotion database for discrete emotion recognition. _IEEE Access_, 7, 12177–12191, 2019.
3.  3\. Batbaatar, E., Li, M., Ryu, K.H., Semantic-emotion neural network for emotion recognition from text. _IEEE Access_, 7, 111866–111878, 2019.
4.  4\. Zhang, Y., Yan, L., Xie, B., Li, X., Zhu, J., Pupil localization algorithm combining convex area voting and model constraint. _Pattern Recognit. Image Anal._, 27, 4, 846–854, 2017.
5.  5\. Meng, H., Bianchi-Berthouze, N., Deng, Y., Cheng, J., Cosmas, J.P., Time-delay neural network for continuous emotional dimension prediction from facial expression sequences. _IEEE Trans. Cybern._, 46, 4, 916–929, Apr. 2016.
6.  6\. Feng, X.U. and Zhang, J.-P., Facial micro expression recognition: A survey. _Acta Automatica Sinica_, 43, 3, 333–348, 2017.
7.  7\. Özerdem, M.S. and Polat, H., Emotion recognition based on EEG features in movie clips with channel selection. _Brain Inf._, 15, 44, 241–252, 2017.
8.  8\. Escalera, S., Baró, X., Guyon, I., Escalante, H.J., Tzimiropoulos, G., Valstar, M., Pantic, M., Cohn, J., Kanade, T., Guest editorial: The computational face. _IEEE Trans. Pattern Anal. Mach. Intell._, 40, 11, 2541–2545, Nov. 2018.
9.  9\. Yu, X., Zhang, S., Yan, Z., Yang, F., Huang, J., Dunbar, N.E., Jensen, M.L., Burgoon, J.K., Metaxas, D.N., Is interactional dissynchrony a clueto deception? Insights from automated analysis of nonverbal visual cues. _IEEE Trans. Cybern._, 45, 3, 492–506, Mar. 2015.
10.  10\. Vella, F., Infantino, I., Scardino, G., Person identification through entropy oriented mean shift clustering of human gaze patterns. _Multimed. Tools Appl._, 76, 2, 2289–2313, Jan. 2017.
11.  11\. Lee, S.H., Plataniotis, K.N.K., Ro, Y.M., Intra-class variation reduction using training expression images for sparse representation basedfacial expression recognition. _IEEE Trans. Affect. Comput._, 5, 3, 340–351, Jul./Sep. 2014.
12.  12\. Ghimire, D., Jeong, S., Lee, J., Park, S.H., Facial expression recognition based on local region specific features and support vector machines. _Multimed. Tools Appl._, 76, 6, 7803–7821, Mar. 2017.
13.  13\. Wang, L., _Behavioral biometrics for human identification: Intelligent applications: Intelligent applications_, IGI Global, Hershey, PA, USA, 2009.
14.  14\. Yan, W.Q., Biometrics for surveillance, in: _Introduction to Intelligent Surveillance_, pp. 107–130, Springer, New York, NY, USA, 2017.
15.  15\. Hess, U., Banse, R., Kappas, A., The intensity of facial expression is determined by underlying affective state and social situation. _J. Personal. Soc Psychol._, 69, 280, 1995.
16.  16\. Dhall, A., Goecke, R., Joshi, J., Hoey, J., Gedeon, T., Emotiw2016: Video and group-level emotion recognition challenges, in: _Proceedingsof the 18th ACM International Conference on Multimodal Interaction_, pp. 427–432, ACM, 2016.
17.  17\. Benitez-Quiroz, C.F., Srinivasan, R., Feng, Q., Wang, Y., Martinez, A.M., Emotionet challenge: Recognition of facial expressions of emotion in the wild. _arXiv preprint arXiv:1703.01210_, 14, 227–229, 2017.
18.  18\. Fasel, B. and Luettin, J., Automatic facial expression analysis: A survey. _Pattern Recognit._, 36, 1, 259–275, 2003.
19.  19\. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., Going deeper with convolutions, in: _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 1–9, 2015.
20.  20\. Hasani, B. and Mahoor, M.H., Spatio-temporal facial expression recognition using convolutional neural networks and conditional randomfields. _2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)_, pp. 790– 795, 2017.
21.  21\. Siddiqi, M.H., Ali, R., Khan, A.M., Kim, E.S., Kim, G.J., Lee, S., Facial expression recognition using active contour-based face detection, facial movement-based feature extraction, and non-linear feature selection. _Multimed. Syst._, 21, 6, 541–555, 2014.
22.  22\. Mlakar, U. and Potočnik, B., Automated facial expression recognition based on histograms of oriented gradient feature vector differences. _Signal Image Video Process._, 9, 1, 245–253, 2015.
23.  23\. Jeni, L.A., Takacs, D., Lorincz, A., High quality facial expression recognition in video streams using shape related information only, in: _Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on_, pp. 2168–2174, 2011.
24.  24\. Erhan, D., Szegedy, C., Toshev, A., Anguelov, D., Scalable object detection using deep neural networks, in: _2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 2155–2162, 2014.
25.  25\. Rychetsky, M., _Algorithms and architectures for machine learning based on regularized neural networks and support vector approaches_, Shaker Verlag GmbH, Germany, ISBN 3826596404, 2001.
26.  26\. Lyons, M., Akamatsu, S., Kamachi, M., Gyoba, J., Coding facial expressions with gabor wavelets, in: _Proceedings of the 3rd International Conference on Face & Gesture Recognition; FG ‘98_, Washington, DC, USA, IEEE Computer Society, p. 200–, ISBN 0-8186-8344-9, 1998.
27.  27\. Kahou, S.E., Froumenty, P., Pal, C., _Facial expression analysis based on high dimensional binary features_, pp. 135–147, Springer International Publishing, Cham, ISBN 978-3-319-16181-5, 2015.

## Note

1.  [\*](https://learning-oreilly-com.ezproxy.christchurchcitylibraries.com/library/view/artificial-intelligence-for/9781119847465/c01.xhtml#rcor1)_Corresponding author_: [cse@rmkec.ac.in](mailto:cse@rmkec.ac.in)